<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jiezi4ai.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="‘Tell me and I forget. Teach me and I remember. Involve me and I learn.’ from Benjamin Franklin  基建篇–vLLM模型选用在 基建篇–AI应用技术框架中提到, 图文理解模型将作为一种基础服务, 以工具的形式供调用, 从而提升AI服务的交互体验. 本文进一步测试和探讨近期(截止2024.03.17)的几">
<meta property="og:type" content="article">
<meta property="og:title" content="基建篇--vLLM模型选用">
<meta property="og:url" content="https://jiezi4ai.github.io/2024/03/19/%E5%9F%BA%E5%BB%BA%E7%AF%87%E2%80%94%E2%80%94vLLM%E6%A8%A1%E5%9E%8B%E9%80%89%E7%94%A8/index.html">
<meta property="og:site_name" content="Jiezi4AI&#39;s Blog">
<meta property="og:description" content="‘Tell me and I forget. Teach me and I remember. Involve me and I learn.’ from Benjamin Franklin  基建篇–vLLM模型选用在 基建篇–AI应用技术框架中提到, 图文理解模型将作为一种基础服务, 以工具的形式供调用, 从而提升AI服务的交互体验. 本文进一步测试和探讨近期(截止2024.03.17)的几">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-f66487673e68e590634951a33173f9d2_1440w.webp">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-351439c545887d9ed44108478333a058_1440w.webp">
<meta property="og:image" content="https://picx.zhimg.com/80/v2-c44258fe3ce6098b52ba8c8a9fdb3235_1440w.png?source=d16d100b">
<meta property="og:image" content="https://picx.zhimg.com/80/v2-aef21989a94b80f1ba6664c3fc2ba534_1440w.png?source=d16d100b">
<meta property="og:image" content="https://picx.zhimg.com/80/v2-60a50132f1e55afab0a442b2d370b61e_1440w.png?source=d16d100b">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-b6e0d5c4f6909ed2d47a9476a72081f6_1440w.png?source=d16d100b">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-ecc451ff913fe9c3eca25672f3f11184_1440w.png?source=d16d100b">
<meta property="og:image" content="https://picx.zhimg.com/80/v2-6b6117eec559a21c2f3627e0d7551b32_1440w.png?source=d16d100b">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-985d20a3cdea0afc8c15ba6b50dd1e05_1440w.png?source=d16d100b">
<meta property="article:published_time" content="2024-03-19T14:19:48.000Z">
<meta property="article:modified_time" content="2024-03-19T14:38:59.558Z">
<meta property="article:author" content="Jiezi">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic3.zhimg.com/80/v2-f66487673e68e590634951a33173f9d2_1440w.webp">

<link rel="canonical" href="https://jiezi4ai.github.io/2024/03/19/%E5%9F%BA%E5%BB%BA%E7%AF%87%E2%80%94%E2%80%94vLLM%E6%A8%A1%E5%9E%8B%E9%80%89%E7%94%A8/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>基建篇--vLLM模型选用 | Jiezi4AI's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Jiezi4AI's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">One Man's Odyssey for AI</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jiezi4ai.github.io/2024/03/19/%E5%9F%BA%E5%BB%BA%E7%AF%87%E2%80%94%E2%80%94vLLM%E6%A8%A1%E5%9E%8B%E9%80%89%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiezi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiezi4AI's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          基建篇--vLLM模型选用
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2024-03-19 22:19:48 / Modified: 22:38:59" itemprop="dateCreated datePublished" datetime="2024-03-19T22:19:48+08:00">2024-03-19</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p>‘Tell me and I forget. Teach me and I remember. Involve me and I learn.’ <em>from Benjamin Franklin</em></p>
</blockquote>
<h1 id="基建篇–vLLM模型选用"><a href="#基建篇–vLLM模型选用" class="headerlink" title="基建篇–vLLM模型选用"></a>基建篇–vLLM模型选用</h1><p>在 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/686736537">基建篇–AI应用技术框架</a>中提到, 图文理解模型将作为一种基础服务, 以工具的形式供调用, 从而提升AI服务的交互体验. 本文进一步测试和探讨近期(截止2024.03.17)的几个不错的较小规模的vLLM模型, 包括<a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepSeek-VL">deepseek-vl-7b</a>, <a target="_blank" rel="noopener" href="https://github.com/OpenBMB/MiniCPM">miniCPM-V-3B</a>, <a target="_blank" rel="noopener" href="https://github.com/OpenBMB/OmniLMM">OmniLMM-12B</a>等，分析相关论文, 评测并选用.</p>
<p><strong>结论先行:</strong> 通过实例测试, 查阅相关文献, 比较训练\推理支持度上(主要更侧重推理), 最终选用了deepseek-vl-7b-chat模型, 我认为它是这个尺寸最好的模型(之一), 功能丰富, 堪称”小钢炮”. 以下将以该模型为主, 对照其它模型分析与评估.</p>
<p><strong>pros:</strong></p>
<ul>
<li>各项图文理解任务综合能力较强, 没有太显著的短板；</li>
<li>对复杂图文和图文结合理解较好, 超出其它模型表现；能够同时支持中英文；</li>
<li>支持多图交叉分析, 除精细的差别对比外, 在常规跨图分析上能有效支持；</li>
<li>有论文支持, 让人比较放心, 论文显示总体的语料选用和训练过程比较solid.<br><strong>cons:</strong></li>
<li>图片中有中文文本时, 反馈内容上幻觉很严重；需要与外部OCR配合使用.</li>
<li>使用的LLM在同规格下中规中矩，算不上顶尖</li>
<li>略显得有一点多舌(over talktive)</li>
<li>暂未支持进一压缩, 如int8&#x2F;int4数据格式， 或awq&#x2F;gtpq&#x2F;gguf等量化部署</li>
</ul>
<h2 id="模型部署"><a href="#模型部署" class="headerlink" title="模型部署"></a>模型部署</h2><p>deepseek-vl模型的部署并不困难, 参照<a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepSeek-VL">它的github</a>上步骤即可完成. 有几点值得留意的地方:</p>
<ul>
<li>已知: serve文件夹下inference.py展示了模型可支持streaming调用</li>
<li>已知: 测试中发现模型支持多图片对比, 传参方式如下:<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;img_query&quot;</span><span class="punctuation">:</span> <span class="string">&quot;第一张图片 &lt;image_placeholder&gt; 和第二张图片 &lt;image_placeholder&gt; 的主要区别是什么？哪一张更好？&quot;</span><span class="punctuation">,</span> # 确保占位符&lt;image_placeholder&gt;与图片一一对应 </span><br><span class="line">    <span class="attr">&quot;img_path_lst&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;/home/taohong/server/tmp/female_01.png&quot;</span><span class="punctuation">,</span> </span><br><span class="line">                     <span class="string">&quot;/home/taohong/server/tmp/female_02.png&quot;</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li>
<li>待确认: 从代码看来, 同时模型可支持以batch的形式传图片做分析, 但此处暂未尝试 (由于我的需求是将vLLM作为服务部署, 本地机器性能有限, 暂无需批量化), 后续如有机会再分析<br>部署下来, 加载模型占用约16GB, 正常推理中总计约占用18-20Gb, 要能压缩下精度就更好了. 我使用Fast-API做了封装, 提供正常调用\streaming调用\多图文交叉理解.</li>
</ul>
<h2 id="论文学习"><a href="#论文学习" class="headerlink" title="论文学习"></a>论文学习</h2><p>个人筛选vLLM的逻辑是, 在效果差别不大的情况下:</p>
<ul>
<li>有论文的 &gt; 没有论文的</li>
<li>从pre-train做起的(除LLM外) &gt; SFT的 &gt; Lora的</li>
<li>有精细数据选用与处理的, 有自有数据的 &gt; 直接使用网络通用数据的<br>回到<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.05525.pdf">论文&#x2F;模型报告</a>上来, 我主要关注的点也在于:</li>
<li>模型如何筛选和处理数据? – 重点关注</li>
<li>模型架构和训练过程有什么特点? – 较关注, 主要是目前主要vLLM架构差别不太大</li>
<li>模型评估效果如何? – 了解即可</li>
</ul>
<p><strong>引言中,</strong> 论文指出, 当前一些开源的vLLM模型表现不佳, 可能原因是: 一来这些模型主要在做instruction tuning, 而忽视了(或没有资源)对pre-training阶段应有的重视； 二是使用的数据集趋同(网络数据集&#x2F;学术上用数据集), 往往与现实世界有一定的距离；三是部分模型在训练中可能出现LLM模型能力退化, 导致图文联合理解能力下降. 以上三点, 我觉得还是蛮切要的.</p>
<p><strong>数据上,</strong> 论文指出在pretraining阶段使用的数据包括了web screenshots, PDFs, OCR, charts, and knowledge-based content (expertise, textbooks)等在内的各种类型, 这一点在后来测试中体现在deepseek模型更”多才多艺”上；在instruction-tuning上模型使用了来自于GPT-4V和Gemini的测试用例, 这点算是惯用做法.<br><img src="https://pic3.zhimg.com/80/v2-f66487673e68e590634951a33173f9d2_1440w.webp" alt="pre-training中使用的数据，还是非常solid的"></p>
<p><strong>架构上,</strong> 在图片部分使用了encoder (图片转向量) + adaptor (转变为向量的图片转文字 ), 再结合原pair中的文本进入LLM中, 总体上和常见的vLLM一致. 具体而言:</p>
<ul>
<li>encoder:<ul>
<li>使用了hybrid vision encoder, 除常用于图像语义表征的SigLIP外, 引入图像专门的自监督编码器SAM-B以保留更多图像信息</li>
<li>为实现对更大尺寸图片的支持(从384<em>384到1024</em>1024), 原则上可以支持更全面和更精细的理解, 当然与之而来的是计算复杂度提升. 模型相应做了一些技术上的创新, 不过我以这块无感, 暂不细探.</li>
</ul>
</li>
<li>adaptor:<ul>
<li>直接使用2层的MLP, 未做更多说明</li>
</ul>
</li>
<li>LLM:<ul>
<li>基座使用了DeepSeek LLM (7B), 查阅<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2401.02954.pdf">其技术报告</a>, 该模型在打榜表现上还是逊色于同尺寸的qwen-1.5-7b及chatGLM3-6B的, 处于中间水平.</li>
</ul>
</li>
</ul>
<p><strong>训练上,</strong> 采取的还是典型的逐步训练.<br><img src="https://pic1.zhimg.com/80/v2-351439c545887d9ed44108478333a058_1440w.webp" alt="采用比较经典的训练三步训练方式"></p>
<ul>
<li>先固定encoder和llm, 训练adaptor将图片encoding结果表达为更易为LLM理解的能力；</li>
<li>接下来, 只固定encoder, 同时优化adaptor和LLM. 由于图片信息在encoder后已比较充分, 而文本信息相对较难学习, 直接混合的多模态信息容出现模型的语言能力退化问题. 本文采取的策略是, 精心控制文本数据占总体的比例, 并使用了一种warm-up的策略来逐步引入vision-language语料的比例. 这一点蛮有创新的, 而且符合能够和直观的认知一致, 我认为是蛮好的创新.</li>
<li>再就是认证训练中的scaling了</li>
<li>最后是使用数据,对encoder, LLM和adaptor进行sft, 主要目的是提升chat的适应能力和指令跟随能力, 仍使用多模态数据+对话文本数据(单独针对LLM)<br><img src="https://picx.zhimg.com/80/v2-c44258fe3ce6098b52ba8c8a9fdb3235_1440w.png?source=d16d100b" alt="SFT阶段使用的数据，特别注意不同数据的构成比例"></li>
</ul>
<h2 id="效果测试"><a href="#效果测试" class="headerlink" title="效果测试"></a>效果测试</h2><p>对模型报告中的一些测试用例印象深刻, 我看到其实各家报告或github上都会有一些案例，其实这些案例蛮有特色的，也很能反映vLLM的能力，因此在这里，我和首先将不同vLLM报告中的样例拿出来，以彼家之矛，攻他家之盾，做一个交叉的测试</p>
<h3 id="交叉测试"><a href="#交叉测试" class="headerlink" title="交叉测试"></a>交叉测试</h3><p><strong>测试一: scratch流程图转python code，</strong>参见报告p.30<br>说实话，初次看到这个我还是有点震惊的，测试中我选用了另一个更复杂一些的，如下图所示：<br><img src="https://picx.zhimg.com/80/v2-aef21989a94b80f1ba6664c3fc2ba534_1440w.png?source=d16d100b" alt="尝试理解Scrach流程图并编写python代码"><br>测试中deepseek-vl-7b理解正确，编程基本OK（可能受限于LLM能力）；OmniLMM-12B与之相仿；miniCPM-V大概理解，但无法编程。</p>
<p><strong>测试二：复杂的可视化数据图</strong><br>选用了一张比测试案例更复杂的可视化数据图，类似报告中p.14 figure4，三个模型表现都不理想，一方面对于关键信息，如横纵坐标、标题的识别不够准确，另一方面不能全面覆盖重要的信息，都没能达到可用的地步。</p>
<p><strong>测试三：按图片写诗，表格截图复原，原神角色识别，即报告p.32项</strong><br>由于这几个使用deepseek报告中的原始案例，因此主要测试OmniLMM-12B和miniCPM-V的表现。其中根据图片写诗上后两者表现不佳，怀疑一方面是二者针对中文语料训练不足，另一方面是可能训练中出现也LLM的能力“退化”，导致图文结合不好。原神角色OmniLMM-12B有误判，可能仍是中文语料不足的问题。表格截图复原三者都没能做到完全精准，其实也不能让人放心使用，看来涉及OCR的还是得更专业的工具来。<br><img src="https://picx.zhimg.com/80/v2-60a50132f1e55afab0a442b2d370b61e_1440w.png?source=d16d100b" alt="根据图片写诗"><br><img src="https://pic1.zhimg.com/80/v2-b6e0d5c4f6909ed2d47a9476a72081f6_1440w.png?source=d16d100b" alt="表格截图识别"><br><img src="https://pic1.zhimg.com/80/v2-ecc451ff913fe9c3eca25672f3f11184_1440w.png?source=d16d100b" alt="原神游戏识别"></p>
<p><strong>测试三：针对[OmniLMM-12B的Github](GitHub - OpenBMB&#x2F;OmniLMM: Large Multi-modal Models for Strong Performance and Efficient Deployment)上的案例进行测试</strong><br><img src="https://picx.zhimg.com/80/v2-6b6117eec559a21c2f3627e0d7551b32_1440w.png?source=d16d100b" alt="来自OmniLMM的测试案例"><br>同样是极富挑战的几张图文理解。其中坡面图deepseek理解正确，miniCPM-V尚可；狗图deepseek理解尚可，miniCPM-V完全不理解；拍照图deepseek误认为是天坛，miniCPM-V只识别为公园；二人图deepseek和miniCPM-V都识别为一个人。</p>
<p><strong>总结来看，deepseek在8个案例中显著出错或明显弱于其它2个模型的情况最少。</strong></p>
<h3 id="其它测试例子"><a href="#其它测试例子" class="headerlink" title="其它测试例子"></a>其它测试例子</h3><p><strong>测试四：含有复杂中文的图片理解</strong><br>直接上含有中文的微信对话截图，问有几个人、对话内容，猜测对话的背景。三者都完全跑题，出现了严重幻觉。这类文本识别，还是要加OCR配合coordinates喂给vLLM才行。话说Qwen-VL-plus和max的API测试发现对于含有中文文本的图片理解非常好，是本身训练使用vLLM具备了很强的OCR能力，还是作为集成方案添加了外部OCR工具呢？</p>
<p><strong>测试五：复杂图文结合的语义理解</strong><br>测试例子如下，询问what is on the image? what kind of emotion it conveys?<br><img src="https://pic1.zhimg.com/80/v2-985d20a3cdea0afc8c15ba6b50dd1e05_1440w.png?source=d16d100b" alt="挑战图文结合的理解能力"><br>三个模型表现都还不错，都能够理解背后的情感，blue的含义，其中deepseek将文字的理解代入图片景物的理解，稍胜一筹。</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/03/18/%E5%BA%94%E7%94%A8%E7%AF%87%E2%80%94%E2%80%94%E4%BA%BA%E6%A0%BC%E5%8C%96AI%EF%BC%9A%E7%A0%94%E7%A9%B6%E4%B8%8E%E5%88%86%E6%9E%90/" rel="prev" title="应用篇——人格化AI：研究与分析">
      <i class="fa fa-chevron-left"></i> 应用篇——人格化AI：研究与分析
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E5%BB%BA%E7%AF%87%E2%80%93vLLM%E6%A8%A1%E5%9E%8B%E9%80%89%E7%94%A8"><span class="nav-number">1.</span> <span class="nav-text">基建篇–vLLM模型选用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2"><span class="nav-number">1.1.</span> <span class="nav-text">模型部署</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.2.</span> <span class="nav-text">论文学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%88%E6%9E%9C%E6%B5%8B%E8%AF%95"><span class="nav-number">1.3.</span> <span class="nav-text">效果测试</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E6%B5%8B%E8%AF%95"><span class="nav-number">1.3.1.</span> <span class="nav-text">交叉测试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E5%AE%83%E6%B5%8B%E8%AF%95%E4%BE%8B%E5%AD%90"><span class="nav-number">1.3.2.</span> <span class="nav-text">其它测试例子</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jiezi</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiezi</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
